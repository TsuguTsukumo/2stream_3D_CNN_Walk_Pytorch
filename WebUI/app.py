# ==============================================================================
# æ¦‚è¦
# ==============================================================================
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Gradioã‚’ä½¿ç”¨ã—ã¦Web UIã‚’æ§‹ç¯‰ã—ã€
# æ­©è¡Œå‹•ç”»ã‹ã‚‰ASDï¼ˆè‡ªé–‰ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ ç—‡ï¼‰ã®å‚¾å‘ã‚’åˆ†é¡ã™ã‚‹ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚
#
# ä¸»ãªæ©Ÿèƒ½ï¼š
# 1. åˆ†é¡ç”¨ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨Hydraè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
# 2. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå‹•ç”»ã‚’1ç§’ã”ã¨ã®ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²ã—ã¾ã™ã€‚
# 3. å„ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰8ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã€YOLOv8ã§äººç‰©ã‚’æ¤œå‡ºãƒ»åˆ‡ã‚Šå‡ºã—ã¾ã™ã€‚
# 4. å„ã‚¯ãƒªãƒƒãƒ—ã«å¯¾ã—ã¦åˆ†é¡æ¨è«–ã‚’è¡Œã„ã€çµæœã‚’ä¸€è¦§ã§è¡¨ç¤ºã—ã¾ã™ã€‚
# 5. Grad-CAMã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒåˆ¤æ–­ã®æ ¹æ‹ ã¨ã—ãŸé ˜åŸŸã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
#
# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼š
# - yolov8n.pt: YOLOv8ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
# - model.ckpt: å­¦ç¿’æ¸ˆã¿ã®Pytorchåˆ†é¡ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
# - make_model.py: `single`ã‚¯ãƒ©ã‚¹ãŒå®šç¾©ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
# - å¯¾å¿œã™ã‚‹config.yaml: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè‡ªå‹•ã§æ¤œå‡ºã•ã‚Œã¾ã™ï¼‰ã€‚
#
# å®Ÿè¡Œæ–¹æ³•ï¼š
# 1. ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ã€
#    å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚
# 2. `python app.py` ã§å®Ÿè¡Œã—ã¦Gradioã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™ã€‚
# 3. è¡¨ç¤ºã•ã‚ŒãŸURLï¼ˆä¾‹: http://127.0.0.1:7860ï¼‰ã«ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚
# ==============================================================================

import gradio as gr
import torch
import cv2
import numpy as np
from ultralytics import YOLO
import os
from collections import OrderedDict
from types import SimpleNamespace
from make_model import single
import yaml
from pathlib import Path
from torchvision import transforms

# ã€è¿½åŠ ã€‘Grad-CAMé–¢é€£ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

# ==========
# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š (GPU or CPU)
# ==========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")


# ==========
# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
# ==========
def build_model(config_path: Path):
    """
    Hydraã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(config.yaml)ã‚’èª­ã¿è¾¼ã¿ã€
    ãã®è¨­å®šã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¦è¿”ã—ã¾ã™ã€‚
    """
    def dict_to_sns(d):
        if not isinstance(d, dict):
            return d
        return SimpleNamespace(**{k: dict_to_sns(v) for k, v in d.items()})

    try:
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {config_path}")
        hparams = dict_to_sns(config_dict)
        model = single(hparams)
        return model
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    except Exception as e:
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# ==========
# Grad-CAM Reshape Transform
# ==========
def reshape_transform(tensor):
    # 5Dãƒ†ãƒ³ã‚½ãƒ« (B, T, C, H, W) ã‚’ 4Dãƒ†ãƒ³ã‚½ãƒ« (B*T, C, H, W) ã«å¤‰æ›
    if len(tensor.shape) == 5:
        b, t, c, h, w = tensor.size()
        result = tensor.reshape(b * t, c, h, w)
        return result
    return tensor

# ==========
# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
# ==========
model_path = "/workspace/project/logs/resnet/single/2025-10-03/05-53-53/fold0/version_0/checkpoints/11-1.75-0.6574.ckpt"
clf_model = None
target_layers = None

if not os.path.exists(model_path):
    print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« '{model_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
else:
    try:
        p = Path(model_path)
        log_dir = p.parent.parent.parent.parent
        config_path = log_dir / ".hydra" / "config.yaml"

        print(f"'{model_path}' ã‹ã‚‰åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        clf_model = build_model(config_path)

        if clf_model is not None:
            checkpoint = torch.load(model_path, map_location=device)
            state_dict = checkpoint.get('state_dict', checkpoint)

            new_state_dict = OrderedDict()
            for k, v in state_dict.items():
                name = k.replace("model.", "", 1)
                new_state_dict[name] = v
            
            clf_model.load_state_dict(new_state_dict, strict=False)
            clf_model = clf_model.to(device)
            clf_model.eval()
            print("âœ… åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨é‡ã¿ã®é©ç”¨ã«æˆåŠŸã—ã¾ã—ãŸã€‚")

            # ã€ä¿®æ­£ã€‘Grad-CAMã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ã‚ˆã‚Šå…·ä½“çš„ã«æŒ‡å®š
            target_layers = [clf_model.resnet_model.layer4[-1]]

    except Exception as e:
        print("="*60)
        print("âŒ åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        print(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
        print("="*60)
        clf_model = None

# ==========
# YOLOãƒ¢ãƒ‡ãƒ«ï¼ˆäººç‰©æ¤œå‡ºç”¨ï¼‰
# ==========
try:
    print("YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    yolo_model = YOLO("yolov8n.pt")
    print("âœ… YOLOv8ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
except Exception as e:
    print(f"âŒ YOLOãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    yolo_model = None


# ==========
# ç”»åƒå‰å‡¦ç†
# ==========
def preprocess_frame(frame):
    if yolo_model is None: return None
    results = yolo_model(frame, conf=0.15, classes=[0])
    if len(results[0].boxes) == 0: return None
    boxes = results[0].boxes.xyxy.cpu().numpy()
    largest_box = max(boxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1]))
    x1, y1, x2, y2 = map(int, largest_box)
    crop = frame[y1:y2, x1:x2]
    if crop.shape[0] == 0 or crop.shape[1] == 0: return None
    target_size = 224
    h0, w0 = crop.shape[:2]
    scale = target_size / max(h0, w0)
    new_w, new_h = int(w0 * scale), int(h0 * scale)
    resized = cv2.resize(crop, (new_w, new_h))
    canvas = np.zeros((target_size, target_size, 3), dtype=np.uint8)
    x_offset = (target_size - new_w) // 2
    y_offset = (target_size - new_h) // 2
    canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
    return canvas

# ==========
# å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ==========
def run_pipeline(video_file, progress=gr.Progress()):
    if yolo_model is None or clf_model is None:
        return [], [], "âŒ ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚"

    cap = cv2.VideoCapture(video_file)
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0:
        fps = 30
        print("è­¦å‘Š: å‹•ç”»ã®FPSãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚30FPSã¨ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")

    all_heatmaps = []
    all_frames_raw = []
    all_results_text = []
    
    frame_number = 0
    clip_duration_frames = int(fps)
    uniform_temporal_subsample_num = 8

    while True:
        frames_for_clip = []
        for _ in range(clip_duration_frames):
            ret, frame = cap.read()
            if not ret: break
            frames_for_clip.append(frame)
        
        if len(frames_for_clip) < clip_duration_frames:
            break
            
        current_second = frame_number / fps
        progress(frame_number / cap.get(cv2.CAP_PROP_FRAME_COUNT), desc=f"{current_second:.1f}ç§’åœ°ç‚¹ã‚’å‡¦ç†ä¸­...")

        num_frames_in_clip = len(frames_for_clip)
        indices = np.linspace(0, num_frames_in_clip - 1, num=uniform_temporal_subsample_num, dtype=int)
        sampled_frames = [frames_for_clip[i] for i in indices]

        frames_raw_clip = []
        for frame in sampled_frames:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            processed = preprocess_frame(frame_rgb)
            if processed is not None:
                frames_raw_clip.append(processed)
        
        if len(frames_raw_clip) < uniform_temporal_subsample_num:
            frame_number += len(frames_for_clip)
            continue

        transform = transforms.Compose([transforms.ToTensor()])
        imgs_tensor = torch.stack([transform(f) for f in frames_raw_clip])
        imgs_tensor = imgs_tensor.to(torch.float32).to(device)
        imgs_tensor = imgs_tensor.unsqueeze(0)

        with torch.no_grad():
            outputs = clf_model(imgs_tensor)
        
        mean_logit = outputs.mean().item()
        pred = 1 if mean_logit > 0 else 0
        prob_for_display = 1 / (1 + np.exp(-mean_logit))
        label = "ASDã®å‚¾å‘ã‚ã‚Š" if pred == 1 else "ASDã®å‚¾å‘ãªã—"
        result_line = f"- {current_second:.1f} - {current_second + 1:.1f}ç§’: {label} (ã‚¹ã‚³ã‚¢: {prob_for_display:.4f})"
        all_results_text.append(result_line)
        
        # --- Grad-CAMè¨ˆç®— ---
        with torch.enable_grad():
            cam = GradCAM(model=clf_model.resnet_model, target_layers=target_layers) # Reshape transformã¯ä¸è¦
            targets = [ClassifierOutputTarget(0) for _ in range(outputs.size(0))]
            
            # 5Dãƒ†ãƒ³ã‚½ãƒ«ã‚’4Dã«å¤‰æ›ã—ã¦CAMã«å…¥åŠ›
            B, T, C, H, W = imgs_tensor.shape
            input_tensor_4d = imgs_tensor.view(B * T, C, H, W)
            
            # ã€ä¿®æ­£ã€‘å¹³æ»‘åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
            grayscale_cam = cam(input_tensor=input_tensor_4d, targets=targets)

        original_images_for_cam = imgs_tensor.squeeze(0).permute(0, 2, 3, 1).cpu().numpy()
        
        for img_np, cam_map in zip(original_images_for_cam, grayscale_cam):
            resized_cam_map = cv2.resize(cam_map, (img_np.shape[1], img_np.shape[0]))
            visualization = show_cam_on_image(img_np, resized_cam_map, use_rgb=True)
            all_heatmaps.append(visualization)
        
        all_frames_raw.extend(frames_raw_clip)
        frame_number += len(frames_for_clip)

    cap.release()

    if not all_results_text:
        return [], [], "âŒ å‹•ç”»ã‹ã‚‰äººç‰©ã‚’æ¤œå‡ºã§ããªã‹ã£ãŸã‹ã€å‡¦ç†ã§ãã‚‹ã‚¯ãƒªãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    final_result_text = "\n".join(all_results_text)
    return all_heatmaps, all_frames_raw, final_result_text

# ==========
# Gradio UI
# ==========
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  ASDåˆ†é¡ãƒ¢ãƒ‡ãƒ« æ¨è«–ãƒ‡ãƒ¢ (1ç§’ã”ã¨)")

    with gr.Row():
        with gr.Column(scale=1):
            video_input = gr.Video(label="æ­©è¡Œå‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
            result_output = gr.Textbox(label="åˆ†é¡çµæœ (1ç§’ã”ã¨)", interactive=False, lines=8)
            submit_btn = gr.Button("å®Ÿè¡Œ", variant="primary")
        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.TabItem("Grad-CAMå¯è¦–åŒ–"):
                    gallery_output = gr.Gallery(label="Grad-CAM", columns=8, height="auto", object_fit="contain")
                with gr.TabItem("å‰å‡¦ç†å¾Œãƒ•ãƒ¬ãƒ¼ãƒ "):
                    preprocessed_output = gr.Gallery(label="å‰å‡¦ç†å¾Œãƒ•ãƒ¬ãƒ¼ãƒ ", columns=8, height="auto", object_fit="contain")

    submit_btn.click(
        fn=run_pipeline,
        inputs=video_input,
        outputs=[gallery_output, preprocessed_output, result_output]
    )
    
    gr.Markdown("--- \n **æ³¨æ„:** ã“ã®ãƒ‡ãƒ¢ã¯ç ”ç©¶ç›®çš„ã§ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€åŒ»å­¦çš„è¨ºæ–­ã«ä»£ã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

if __name__ == "__main__":
    demo.launch()

