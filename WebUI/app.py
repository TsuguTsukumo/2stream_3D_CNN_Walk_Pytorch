# ==============================================================================
# æ¦‚è¦
# ==============================================================================
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Gradioã‚’ä½¿ç”¨ã—ã¦Web UIã‚’æ§‹ç¯‰ã—ã€
# æ­£é¢ãƒ»å´é¢ã®æ­©è¡Œå‹•ç”»ã‹ã‚‰ASDå‚¾å‘ã‚’åˆ†é¡ã™ã‚‹ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚
#
# ä¸»ãªæ©Ÿèƒ½ï¼š
# 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆSingle/Fusionï¼‰ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚
# 2. å‹•ç”»ã‚’1ç§’ã”ã¨ã®ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²ã—ã€YOLOv8ã§äººç‰©ã‚’æ¤œå‡ºãƒ»åˆ‡ã‚Šå‡ºã—ã¾ã™ã€‚
# 3. å„ã‚¯ãƒªãƒƒãƒ—ã«å¯¾ã—ã¦åˆ†é¡æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚
# 4. å…¨ã‚¯ãƒªãƒƒãƒ—ã®çµæœã‚’çµ±åˆã—ã€å‹•ç”»å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ã‚¿ãƒ–ã«è¡¨ç¤ºã—ã¾ã™ã€‚
# 5. æœ€ã‚‚ã‚¹ã‚³ã‚¢ãŒé«˜ã‹ã£ãŸåŒºé–“ã¨ã€æ³¨ç›®éƒ¨ä½ã®å‚¾å‘ã‚’ã‚¤ãƒ©ã‚¹ãƒˆã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
# 6. Singleãƒ¢ãƒ‡ãƒ«é¸æŠæ™‚ã«ã¯ã€Grad-CAMã§åˆ¤æ–­æ ¹æ‹ ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
#
# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼š
# - yolov8n.pt: YOLOv8ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
# - å„ç¨®ãƒ¢ãƒ‡ãƒ«ã®.ckptãƒ•ã‚¡ã‚¤ãƒ«: MODEL_PATHSã«ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚
# - make_model.py: å„ç¨®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ãŒå®šç¾©ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã€‚
# ==============================================================================

import gradio as gr
import torch
import cv2
import numpy as np
from ultralytics import YOLO
import os
from collections import OrderedDict, Counter
from types import SimpleNamespace
from make_model import single, early_fusion, late_fusion, slow_fusion
import yaml
from pathlib import Path
from torchvision import transforms

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

# ==========
# ã€é‡è¦ã€‘ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# ==========
MODEL_PATHS = {
    "single_front": "/workspace/project/logs/resnet/single/2025-10-03/05-53-53/fold0/version_0/checkpoints/11-1.75-0.6574.ckpt",
    "single_side": "/workspace/logs/resnet/single/2025-10-06/04-29-41/fold1/version_0/checkpoints/12-0.96-0.7715.ckpt",
    "early_fusion": "/workspace/project/logs/resnet/early_fusion/2025-02-03/05-15-18/fold2/version_0/checkpoints/16-0.60-0.7234.ckpt",
    "late_fusion": "/workspace/logs/resnet/late_fusion/2025-10-07/05-01-13/fold4/version_0/checkpoints/1-0.72-0.6206.ckpt",
    "slow_fusion": "/workspace/project/logs/resnet/slow_fusion/2025-02-03/05-15-18/fold0/version_0/checkpoints/8-0.94-0.5779.ckpt",
}
# ==========
# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š (GPU or CPU)
# ==========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")


# ==========
# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
# ==========
def build_model(config_path: Path):
    def dict_to_sns(d):
        if not isinstance(d, dict): return d
        return SimpleNamespace(**{k: dict_to_sns(v) for k, v in d.items()})

    try:
        with open(config_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        hparams = dict_to_sns(config_dict)
        model_type_from_config = hparams.train.experiment

        if model_type_from_config == 'single':
            model = single(hparams)
        elif model_type_from_config == 'early_fusion':
            model = early_fusion(hparams)
        elif model_type_from_config == 'late_fusion':
            model = late_fusion(hparams)
        elif model_type_from_config == 'slow_fusion':
            model = slow_fusion(hparams)
        else:
            raise ValueError(f"ä¸æ˜ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type_from_config}")
        return model
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_model(model_type):
    model_path = MODEL_PATHS.get(model_type)
    if not model_path or not os.path.exists(model_path):
        print(f"è­¦å‘Š: '{model_type}'ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    
    try:
        p = Path(model_path)
        log_dir = p.parent.parent.parent.parent
        config_path = log_dir / ".hydra" / "config.yaml"

        model = build_model(config_path)
        if model is None: raise ValueError("ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã«å¤±æ•—ã€‚")

        checkpoint = torch.load(model_path, map_location=device)
        state_dict = checkpoint.get('state_dict', checkpoint)
        new_state_dict = OrderedDict()
        for k, v in state_dict.items():
            name = k.replace("model.", "", 1)
            new_state_dict[name] = v
        model.load_state_dict(new_state_dict, strict=False)
        model = model.to(device)
        model.eval()
        print(f"âœ… '{model_type}'ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return model
    except Exception as e:
        print(f"âŒ '{model_type}'ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ==========
# YOLOãƒ¢ãƒ‡ãƒ«ï¼ˆäººç‰©æ¤œå‡ºç”¨ï¼‰
# ==========
try:
    print("YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    yolo_model = YOLO("yolov8n.pt")
    print("âœ… YOLOv8ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
except Exception as e:
    print(f"âŒ YOLOãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    yolo_model = None


# ==========
# ç”»åƒå‰å‡¦ç†
# ==========
def preprocess_frame(frame, crop_mode):
    if yolo_model is None: return None, None
    results = yolo_model(frame, conf=0.15, classes=[0])
    if len(results[0].boxes) == 0: return None, None
    boxes = results[0].boxes.xyxy.cpu().numpy()
    largest_box = max(boxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1]))
    x1, y1, x2, y2 = map(int, largest_box)

    if crop_mode == "ä¸‹åŠèº«":
        y1 = y1 + (y2 - y1) // 2
        
    crop = frame[y1:y2, x1:x2]
    if crop.shape[0] == 0 or crop.shape[1] == 0: return None, None
    target_size = 224
    h0, w0 = crop.shape[:2]
    scale = target_size / max(h0, w0)
    new_w, new_h = int(w0 * scale), int(h0 * scale)
    resized = cv2.resize(crop, (new_w, new_h))
    canvas = np.zeros((target_size, target_size, 3), dtype=np.uint8)
    x_offset = (target_size - new_w) // 2
    y_offset = (target_size - new_h) // 2
    canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
    
    bbox_on_canvas = (x_offset, y_offset, new_w, new_h)
    return canvas, bbox_on_canvas

# ==========
# Grad-CAMç”Ÿæˆã¨éƒ¨ä½ç‰¹å®š
# ==========
def get_heatmaps_and_focus(cam_model, input_tensor_5d, bboxes_on_canvas):
    if cam_model is None: 
        return [np.zeros((224, 224, 3), dtype=np.uint8)] * 8, "N/A", {"é ­éƒ¨": 0, "èƒ´ä½“": 0, "è„šéƒ¨": 0}
    
    heatmaps_clip = []
    clip_sums = {"é ­éƒ¨": 0.0, "èƒ´ä½“": 0.0, "è„šéƒ¨": 0.0}

    with torch.enable_grad():
        cam_target_model = cam_model.resnet_model
        target_layers = [cam_target_model.layer4[-1]]
        cam = GradCAM(model=cam_target_model, target_layers=target_layers)
        B, T, C, H, W = input_tensor_5d.shape
        input_tensor_4d = input_tensor_5d.view(B * T, C, H, W)
        targets = [ClassifierOutputTarget(0)] * (B * T)
        grayscale_cam = cam(input_tensor=input_tensor_4d, targets=targets)
    
    original_images_for_cam = input_tensor_5d.squeeze(0).permute(0, 2, 3, 1).cpu().numpy()
    
    for i, (img_np, cam_map) in enumerate(zip(original_images_for_cam, grayscale_cam)):
        resized_cam_map = cv2.resize(cam_map, (img_np.shape[1], img_np.shape[0]))
        visualization = show_cam_on_image(img_np, resized_cam_map, use_rgb=True)
        heatmaps_clip.append(visualization)
        
        bbox_x, bbox_y, bbox_w, bbox_h = map(int, bboxes_on_canvas[i])
        if bbox_h > 0:
            head_end_y = bbox_y + int(bbox_h * 0.25)
            torso_end_y = bbox_y + int(bbox_h * 0.6)
            legs_end_y = bbox_y + bbox_h
            
            clip_sums["é ­éƒ¨"] += np.sum(resized_cam_map[bbox_y:head_end_y, bbox_x:bbox_x+bbox_w])
            clip_sums["èƒ´ä½“"] += np.sum(resized_cam_map[head_end_y:torso_end_y, bbox_x:bbox_x+bbox_w])
            clip_sums["è„šéƒ¨"] += np.sum(resized_cam_map[torso_end_y:legs_end_y, bbox_x:bbox_x+bbox_w])

    focus_part = max(clip_sums, key=clip_sums.get) if sum(clip_sums.values()) > 0 else "ä¸æ˜"

    return heatmaps_clip, focus_part, clip_sums

# ==========
# æ³¨ç›®éƒ¨ä½ã‚¤ãƒ©ã‚¹ãƒˆç”Ÿæˆ
# ==========
def create_focus_illustration_html(focus_part, view_type, is_asd):
    highlight_color = "#E53E3E" if is_asd else "#D3D3D3"
    default_color = "#D3D3D3"

    fills = {"head": default_color, "torso": default_color, "legs": default_color}
    if focus_part == "é ­éƒ¨": fills["head"] = highlight_color
    elif focus_part == "èƒ´ä½“": fills["torso"] = highlight_color
    elif focus_part == "è„šéƒ¨": fills["legs"] = highlight_color

    if view_type == "front":
        svg_code = f"""<svg width="150" height="400" viewBox="0 0 150 400" xmlns="http://www.w3.org/2000/svg">
            <style>.body {{ stroke: #555; stroke-width:2; }}</style><title>æ­£é¢å›³</title>
            <g id="figure-front">
                <circle id="head-front" class="body" cx="75" cy="50" r="35" fill="{fills['head']}"/>
                <rect id="torso-front" class="body" x="40" y="90" width="70" height="120" rx="10" fill="{fills['torso']}"/>
                <rect id="arm-front-left" class="body" x="15" y="95" width="20" height="110" rx="10" fill="{fills['torso']}"/>
                <rect id="arm-front-right" class="body" x="115" y="95" width="20" height="110" rx="10" fill="{fills['torso']}"/>
                <rect id="legs-front-left" class="body" x="40" y="215" width="32" height="150" rx="10" fill="{fills['legs']}"/>
                <rect id="legs-front-right" class="body" x="78" y="215" width="32" height="150" rx="10" fill="{fills['legs']}"/>
            </g>
            <text x="75" y="390" font-family="sans-serif" font-size="20" text-anchor="middle">æ­£é¢</text></svg>"""
    else: # side view
        # viewBoxã‚’åºƒã’ã€å…¨ä½“ã‚’ä¸­å¤®ã«é…ç½®ã™ã‚‹ã‚ˆã†èª¿æ•´
        svg_code = f"""<svg width="180" height="400" viewBox="0 0 180 400" xmlns="http://www.w3.org/2000/svg">
            <style>.body {{ stroke: #555; stroke-width:2; }}</style><title>å´é¢å›³</title>
            <g id="figure-side" transform="translate(20, 0)"> <!-- å…¨ä½“ã‚’ã•ã‚‰ã«å³ã«ãšã‚‰ã™ --><!-- Far limbs (opacity reduced) --><rect id="arm-side-far" class="body" x="65" y="95" width="20" height="110" rx="10" transform="rotate(30, 75, 100)" fill="{fills['torso']}" opacity="0.7"/>
                <rect id="leg-side-far" class="body" x="63" y="210" width="25" height="150" rx="10" transform="rotate(-20, 75, 215)" fill="{fills['legs']}" opacity="0.7"/>

                <!-- Body --><circle id="head-side" class="body" cx="75" cy="50" r="35" fill="{fills['head']}"/>
                <rect id="torso-side" class="body" x="60" y="90" width="50" height="120" rx="10" fill="{fills['torso']}"/>
                
                <!-- Near limbs --><rect id="arm-side-near" class="body" x="65" y="95" width="20" height="110" rx="10" transform="rotate(-40, 75, 100)" fill="{fills['torso']}"/>
                <rect id="leg-side-near" class="body" x="63" y="210" width="25" height="150" rx="10" transform="rotate(30, 75, 215)" fill="{fills['legs']}"/>
            </g>
            <text x="75" y="390" font-family="sans-serif" font-size="20" text-anchor="middle">å´é¢</text></svg>"""
    return f"<div style='text-align:center;'>{svg_code}</div>"

# ==========
# å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ==========
def run_pipeline(model_type, video_front, video_side, crop_mode, progress=gr.Progress()):
    def make_error_return(message):
        return ([], [], [], [], 0, None, None, message, None, None,
                gr.update(interactive=False), gr.update(interactive=False), "", gr.update())

    progress(0, desc=f"'{model_type}'ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    clf_model = load_model(model_type)
    if clf_model is None: return make_error_return(f"âŒ '{model_type}'ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    is_fusion, is_single = "fusion" in model_type, "single" in model_type
    single_front_cam_model, single_side_cam_model = None, None
    if is_fusion:
        progress(0.1, desc="CAMç”¨Singleãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        single_front_cam_model, single_side_cam_model = load_model("single_front"), load_model("single_side")

    cap_front, cap_side = None, None
    if is_fusion or "front" in model_type:
        if not video_front: return make_error_return("âŒ æ­£é¢å‹•ç”»ãŒå¿…è¦ã§ã™ã€‚")
        cap_front = cv2.VideoCapture(video_front)
    if is_fusion or "side" in model_type:
        if not video_side: return make_error_return("âŒ å´é¢å‹•ç”»ãŒå¿…è¦ã§ã™ã€‚")
        cap_side = cv2.VideoCapture(video_side)

    fps = cap_front.get(cv2.CAP_PROP_FPS) if cap_front else cap_side.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30
    
    total_frames = float('inf')
    if cap_front: total_frames = min(total_frames, cap_front.get(cv2.CAP_PROP_FRAME_COUNT))
    if cap_side: total_frames = min(total_frames, cap_side.get(cv2.CAP_PROP_FRAME_COUNT))

    all_composite_images, time_interval_labels, all_scores, all_focus_parts = [], [], [], []
    all_clip_sums_front, all_clip_sums_side = [], []
    frame_number, clip_duration_frames, uniform_temporal_subsample_num = 0, int(fps), 8

    while True:
        frames_for_clip_front, frames_for_clip_side = [], []
        for _ in range(clip_duration_frames):
            ret_front, ret_side = True, True
            if cap_front: ret_front, frame_front = cap_front.read()
            if cap_side: ret_side, frame_side = cap_side.read()
            if not ret_front or not ret_side: break
            if cap_front: frames_for_clip_front.append(frame_front)
            if cap_side: frames_for_clip_side.append(frame_side)
        
        if (cap_front and len(frames_for_clip_front) < clip_duration_frames) or \
           (cap_side and len(frames_for_clip_side) < clip_duration_frames):
            break
            
        current_second = frame_number / fps
        progress(frame_number / total_frames, desc=f"{current_second:.1f}ç§’åœ°ç‚¹ã‚’å‡¦ç†ä¸­...")

        def sample_and_process_clip(clip_frames, crop_mode):
            indices = np.linspace(0, len(clip_frames) - 1, num=uniform_temporal_subsample_num, dtype=int)
            processed_clip, bboxes = [], []
            for i in indices:
                frame, bbox = preprocess_frame(cv2.cvtColor(clip_frames[i], cv2.COLOR_BGR2RGB), crop_mode)
                if frame is not None:
                    processed_clip.append(frame)
                    bboxes.append(bbox)
            return processed_clip, bboxes

        frames_raw_clip_front, bboxes_front = sample_and_process_clip(frames_for_clip_front, crop_mode) if cap_front else ([], [])
        frames_raw_clip_side, bboxes_side = sample_and_process_clip(frames_for_clip_side, crop_mode) if cap_side else ([], [])

        if (cap_front and len(frames_raw_clip_front) < uniform_temporal_subsample_num) or \
           (cap_side and len(frames_raw_clip_side) < uniform_temporal_subsample_num):
            frame_number += clip_duration_frames
            continue

        transform = transforms.Compose([transforms.ToTensor()])
        imgs_tensor_front = torch.stack([transform(f) for f in frames_raw_clip_front]).to(torch.float32).to(device).unsqueeze(0) if cap_front else None
        imgs_tensor_side = torch.stack([transform(f) for f in frames_raw_clip_side]).to(torch.float32).to(device).unsqueeze(0) if cap_side else None

        with torch.no_grad():
            if is_fusion: outputs = clf_model(imgs_tensor_front, imgs_tensor_side)
            elif "front" in model_type: outputs = clf_model(imgs_tensor_front)
            else: outputs = clf_model(imgs_tensor_side)
        
        logit = outputs.mean().item()
        prob_for_display = 1 / (1 + np.exp(-logit))
        interval_label, _ = f"{current_second:.1f}-{current_second + 1:.1f}s", time_interval_labels.append(f"{current_second:.1f}-{current_second + 1:.1f}s")
        all_scores.append(prob_for_display)
        
        if is_single:
            cam_model, input_tensor, bboxes, frames_for_display = (clf_model, imgs_tensor_front, bboxes_front, frames_raw_clip_front) if "front" in model_type else (clf_model, imgs_tensor_side, bboxes_side, frames_raw_clip_side)
            heatmaps_clip, focus_part, clip_sums = get_heatmaps_and_focus(cam_model, input_tensor, bboxes)
            all_focus_parts.append(focus_part)
            if "front" in model_type: all_clip_sums_front.append(clip_sums)
            else: all_clip_sums_side.append(clip_sums)
            top_row, bottom_row = cv2.hconcat(frames_for_display), cv2.hconcat(heatmaps_clip)
            composite_image = cv2.vconcat([top_row, bottom_row])
        elif is_fusion:
            heatmaps_front, focus_front, clip_sums_front = get_heatmaps_and_focus(single_front_cam_model, imgs_tensor_front, bboxes_front)
            heatmaps_side, focus_side, clip_sums_side = get_heatmaps_and_focus(single_side_cam_model, imgs_tensor_side, bboxes_side)
            focus_part = f"æ­£é¢:{focus_front}, å´é¢:{focus_side}"
            all_focus_parts.append(focus_part)
            all_clip_sums_front.append(clip_sums_front)
            all_clip_sums_side.append(clip_sums_side)
            row1, row2, row3, row4 = cv2.hconcat(frames_raw_clip_front), cv2.hconcat(heatmaps_front), cv2.hconcat(frames_raw_clip_side), cv2.hconcat(heatmaps_side)
            composite_image = cv2.vconcat([row1, row2, row3, row4])
        
        all_composite_images.append(composite_image)
        frame_number += clip_duration_frames

    if cap_front: cap_front.release()
    if cap_side: cap_side.release()

    if not all_scores: return make_error_return("âŒ å‡¦ç†ã§ãã‚‹ã‚¯ãƒªãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    overall_score = np.mean(all_scores)
    overall_label, label_color = ("ASDã®å‚¾å‘ã‚ã‚Š", "#E53E3E") if overall_score > 0.5 else ("ASDã®å‚¾å‘ãªã—", "#38A169")
    most_abnormal_index = np.argmax(all_scores)
    most_abnormal_image = all_composite_images[most_abnormal_index]

    def get_focus_summary_and_part(clip_sums_list):
        if not clip_sums_list: return "N/A", "N/A"
        total_sums = {"é ­éƒ¨": 0.0, "èƒ´ä½“": 0.0, "è„šéƒ¨": 0.0}
        for clip_sums in clip_sums_list:
            for part, value in clip_sums.items(): total_sums[part] += value
        if sum(total_sums.values()) == 0: return "N/A", "N/A"
        most_common_part = max(total_sums, key=total_sums.get)
        summary = f"**{most_common_part}**"
        if overall_score > 0.5: summary = f"<span style='color:{label_color};'>{summary}</span>"
        return summary, most_common_part

    focus_summary_front, most_common_front = get_focus_summary_and_part(all_clip_sums_front)
    focus_summary_side, most_common_side = get_focus_summary_and_part(all_clip_sums_side)

    summary_markdown = f"""<div style='text-align:center; padding: 10px; border-radius: 5px; background-color: #f9f9f9;'>
        <p style='margin-bottom: 5px;'>æœ€çµ‚åˆ¤å®š</p><h2 style='color:{label_color}; margin-top: 0px;'>{overall_label}</h2><hr>
        <div style='text-align:left;'>
        **å¹³å‡ã‚¹ã‚³ã‚¢:** {overall_score:.4f}<br>
        **ä¸»ãªæ³¨ç›®éƒ¨ä½ (æ­£é¢):** {focus_summary_front}<br>
        **ä¸»ãªæ³¨ç›®éƒ¨ä½ (å´é¢):** {focus_summary_side}<br><br>
        **ã€æœ€ã‚‚é¡•è‘—ãªåŒºé–“ã€‘**<br>
        **åŒºé–“:** {time_interval_labels[most_abnormal_index]}<br>
        **ã‚¹ã‚³ã‚¢:** {all_scores[most_abnormal_index]:.4f}</div></div>"""

    display_label_text = f"åŒºé–“: {time_interval_labels[0]} | ã‚¹ã‚³ã‚¢: {all_scores[0]:.4f} | æ³¨ç›®éƒ¨ä½: {all_focus_parts[0]}"
    image_label = "ä¸Šæ®µ: å…¥åŠ›, ä¸‹æ®µ: CAM" if is_single else "1,3æ®µç›®: å…¥åŠ›(æ­£,å´), 2,4æ®µç›®: CAM(æ­£,å´)"
    
    return (all_composite_images, time_interval_labels, all_scores, all_focus_parts, 0, all_composite_images[0],
            most_abnormal_image, summary_markdown, 
            create_focus_illustration_html(most_common_front, "front", overall_score > 0.5),
            create_focus_illustration_html(most_common_side, "side", overall_score > 0.5),
            gr.update(interactive=False), gr.update(interactive=len(all_composite_images) > 1), display_label_text, gr.update(label=image_label))

def navigate_images(current_index, direction, all_images, all_intervals, all_scores, all_focus_parts):
    if not all_images: return 0, None, gr.update(interactive=False), gr.update(interactive=False), ""
    max_index, new_index = len(all_images) - 1, max(0, min(current_index + direction, len(all_images) - 1))
    image, prev_btn, next_btn = all_images[new_index], gr.update(interactive=new_index > 0), gr.update(interactive=new_index < max_index)
    label = f"åŒºé–“: {all_intervals[new_index]} | ã‚¹ã‚³ã‚¢: {all_scores[new_index]:.4f} | æ³¨ç›®éƒ¨ä½: {all_focus_parts[new_index]}"
    return new_index, image, prev_btn, next_btn, label

# ==========
# Gradio UI
# ==========
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  ASDåˆ†é¡ãƒ¢ãƒ‡ãƒ« æ¨è«–ãƒ‡ãƒ¢")

    image_state, interval_state, scores_state, focus_parts_state, current_index_state = gr.State([]), gr.State([]), gr.State([]), gr.State([]), gr.State(0)

    with gr.Row():
        with gr.Column(scale=1):
            model_type_input = gr.Radio(list(MODEL_PATHS.keys()), value="early_fusion", label="ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
            video_input_front = gr.Video(label="æ­£é¢å‹•ç”» (Single/Fusionã§åˆ©ç”¨)")
            video_input_side = gr.Video(label="å´é¢å‹•ç”» (Single/Fusionã§åˆ©ç”¨)")
            crop_mode_input = gr.Radio(["å…¨èº«", "ä¸‹åŠèº«"], value="å…¨èº«", label="åˆ‡ã‚Šå‡ºã—ç¯„å›²")
            submit_btn = gr.Button("å®Ÿè¡Œ", variant="primary")

        with gr.Column(scale=2):
            with gr.Tabs(selected=0) as tabs: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å…¨ä½“ã‚µãƒãƒªãƒ¼(id=0)ã«è¨­å®š
                with gr.TabItem("å…¨ä½“ã‚µãƒãƒªãƒ¼", id=0):
                    with gr.Row():
                        summary_output = gr.Markdown()
                        with gr.Row():
                            illustration_front = gr.HTML()
                            illustration_side = gr.HTML()
                    abnormal_image_output = gr.Image(label="æœ€ã‚‚é¡•è‘—ãªåŒºé–“ã®å¯è¦–åŒ–", interactive=False)
                with gr.TabItem("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¡¨ç¤º", id=1):
                    with gr.Row():
                        prev_btn = gr.Button("å‰ã®åŒºé–“", interactive=False)
                        display_label = gr.Textbox(label="è¡¨ç¤ºä¸­ã®æƒ…å ±", interactive=False, text_align="center")
                        next_btn = gr.Button("æ¬¡ã®åŒºé–“", interactive=False)
                    image_output = gr.Image(label="ç§’ã”ã¨ã®å¯è¦–åŒ–çµæœ", interactive=False)

    submit_btn.click(
        fn=run_pipeline,
        inputs=[model_type_input, video_input_front, video_input_side, crop_mode_input],
        outputs=[image_state, interval_state, scores_state, focus_parts_state, current_index_state, image_output,
                 abnormal_image_output, summary_output, illustration_front, illustration_side,
                 prev_btn, next_btn, display_label, image_output]
    )
    
    prev_btn.click(
        fn=lambda idx, imgs, intervals, scores, parts: navigate_images(idx, -1, imgs, intervals, scores, parts),
        inputs=[current_index_state, image_state, interval_state, scores_state, focus_parts_state],
        outputs=[current_index_state, image_output, prev_btn, next_btn, display_label]
    )

    next_btn.click(
        fn=lambda idx, imgs, intervals, scores, parts: navigate_images(idx, 1, imgs, intervals, scores, parts),
        inputs=[current_index_state, image_state, interval_state, scores_state, focus_parts_state],
        outputs=[current_index_state, image_output, prev_btn, next_btn, display_label]
    )
    
    gr.Markdown("--- \n **æ³¨æ„:** ã“ã®ãƒ‡ãƒ¢ã¯ç ”ç©¶ç›®çš„ã§ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€åŒ»å­¦çš„è¨ºæ–­ã«ä»£ã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

if __name__ == "__main__":
    demo.launch()